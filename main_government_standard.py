"""
ì •ë¶€/ê³µê³µê¸°ê´€ í‘œì¤€ ë°ì´í„° ì²˜ë¦¬ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
PDF â†’ JSON â†’ ì •ê·œí™” â†’ DB ì ì¬ â†’ ì‹œê°í™” â†’ ê²€ì¦
"""
import os
import json
from pathlib import Path
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import Dict, Any

# ëª¨ë“ˆ ì„í¬íŠ¸
from normalize_government_standard import GovernmentStandardNormalizer
from load_government_standard_db import GovernmentStandardDBLoader
from config import MYSQL_CONFIG

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


class GovernmentDataPipeline:
    """ì •ë¶€ í‘œì¤€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.input_dir = Path("output")  # JSON íŒŒì¼ ìœ„ì¹˜
        self.output_dir = Path("normalized_output_government")
        self.visualization_dir = Path("visualization_government")
        self.visualization_dir.mkdir(exist_ok=True)
        
        # í†µê³„
        self.pipeline_stats = {
            'start_time': datetime.now(),
            'json_files_processed': 0,
            'total_tables_extracted': 0,
            'normalized_records': 0,
            'db_records_loaded': 0,
            'verification_passed': False,
            'errors': []
        }
    
    def run_normalization(self) -> Dict[str, int]:
        """ì •ê·œí™” ì‹¤í–‰"""
        logger.info("ğŸ”„ ì •ê·œí™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        normalized_stats = {
            'files_processed': 0,
            'total_records': 0,
            'records_by_type': {}
        }
        
        # JSON íŒŒì¼ ì°¾ê¸°
        json_files = list(self.input_dir.glob("*.json"))
        
        if not json_files:
            logger.error("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return normalized_stats
        
        for json_file in json_files:
            try:
                logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {json_file.name}")
                
                # ì •ê·œí™” ì‹¤í–‰
                normalizer = GovernmentStandardNormalizer(
                    str(json_file),
                    str(self.output_dir)
                )
                
                # JSON ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ì •ê·œí™” ì²˜ë¦¬
                normalizer.normalize(data)
                
                # CSV ì €ì¥
                normalizer.save_to_csv()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                normalized_stats['files_processed'] += 1
                
                # ë ˆì½”ë“œ ìˆ˜ ì¹´ìš´íŠ¸
                for table_name in normalizer.data.keys():
                    count = len(normalizer.data[table_name])
                    if table_name not in normalized_stats['records_by_type']:
                        normalized_stats['records_by_type'][table_name] = 0
                    normalized_stats['records_by_type'][table_name] += count
                    normalized_stats['total_records'] += count
                
                logger.info(f"âœ… {json_file.name} ì •ê·œí™” ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ {json_file.name} ì •ê·œí™” ì‹¤íŒ¨: {e}")
                self.pipeline_stats['errors'].append(f"ì •ê·œí™” ì‹¤íŒ¨: {json_file.name}")
        
        self.pipeline_stats['json_files_processed'] = normalized_stats['files_processed']
        self.pipeline_stats['normalized_records'] = normalized_stats['total_records']
        
        return normalized_stats
    
    def load_to_database(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì ì¬"""
        logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ì‹œì‘...")
        
        # DB ì„¤ì •
        db_config = MYSQL_CONFIG.copy()
        db_config['database'] = 'government_standard'
        
        # ì ì¬ ì‹¤í–‰
        loader = GovernmentStandardDBLoader(db_config, str(self.output_dir))
        
        try:
            loader.connect()
            loader.drop_existing_tables()
            loader.create_tables()
            loader.load_all_tables()
            
            # ê²€ì¦
            verification = loader.verify_data_integrity()
            
            self.pipeline_stats['db_records_loaded'] = loader.load_stats['total_records']
            
            return verification
            
        except Exception as e:
            logger.error(f"âŒ DB ì ì¬ ì‹¤íŒ¨: {e}")
            self.pipeline_stats['errors'].append(f"DB ì ì¬ ì‹¤íŒ¨: {str(e)}")
            return {}
            
        finally:
            loader.close()
    
    def visualize_data(self, normalized_stats: Dict, db_verification: Dict):
        """ë°ì´í„° ì‹œê°í™”"""
        logger.info("ğŸ“Š ë°ì´í„° ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # 1. ì •ê·œí™” í†µê³„ ì‹œê°í™”
        self._plot_normalization_stats(normalized_stats)
        
        # 2. DB ì ì¬ í†µê³„ ì‹œê°í™”
        self._plot_db_stats(db_verification)
        
        # 3. ë°ì´í„° ì™„ì „ì„± ì‹œê°í™”
        self._plot_data_completeness(db_verification)
        
        # 4. íŒŒì´í”„ë¼ì¸ ìš”ì•½ ì‹œê°í™”
        self._plot_pipeline_summary()
        
        logger.info(f"âœ… ì‹œê°í™” ì™„ë£Œ - {self.visualization_dir}ì— ì €ì¥ë¨")
    
    def _plot_normalization_stats(self, stats: Dict):
        """ì •ê·œí™” í†µê³„ ì‹œê°í™”"""
        if not stats.get('records_by_type'):
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜
        tables = list(stats['records_by_type'].keys())
        counts = list(stats['records_by_type'].values())
        
        ax1.bar(range(len(tables)), counts, color='steelblue')
        ax1.set_xticks(range(len(tables)))
        ax1.set_xticklabels(tables, rotation=45, ha='right')
        ax1.set_title('Records by Table Type')
        ax1.set_ylabel('Record Count')
        ax1.grid(True, alpha=0.3)
        
        # íŒŒì´ ì°¨íŠ¸
        colors = plt.cm.Set3(range(len(tables)))
        ax2.pie(counts, labels=tables, autopct='%1.1f%%', colors=colors)
        ax2.set_title('Data Distribution')
        
        plt.tight_layout()
        plt.savefig(self.visualization_dir / 'normalization_stats.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_db_stats(self, verification: Dict):
        """DB ì ì¬ í†µê³„ ì‹œê°í™”"""
        if not verification.get('normalized_counts'):
            return
        
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        # ì •ê·œí™” í…Œì´ë¸” ë°ì´í„°
        tables = list(verification['normalized_counts'].keys())
        counts = list(verification['normalized_counts'].values())
        
        # ë°” ì°¨íŠ¸
        bars = ax.bar(tables, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        
        # ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height):,}',
                   ha='center', va='bottom')
        
        ax.set_title('Database Load Statistics', fontsize=14, fontweight='bold')
        ax.set_ylabel('Record Count')
        ax.set_xlabel('Table Name')
        ax.grid(True, alpha=0.3, axis='y')
        
        # ì›ë³¸ ë°ì´í„° ìˆ˜ ì¶”ê°€
        ax.axhline(y=verification.get('raw_data_count', 0), 
                  color='red', linestyle='--', alpha=0.5, 
                  label=f"Raw Data: {verification.get('raw_data_count', 0):,}")
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.visualization_dir / 'db_load_stats.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_data_completeness(self, verification: Dict):
        """ë°ì´í„° ì™„ì „ì„± ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. ë°ì´í„° ì™„ì „ì„± ì§€í‘œ
        total_projects = verification.get('total_sub_projects', 0)
        missing_data = len(verification.get('missing_data', []))
        complete_projects = total_projects - missing_data
        
        if total_projects > 0:
            completeness = (complete_projects / total_projects) * 100
            
            # ë„ë„› ì°¨íŠ¸
            sizes = [complete_projects, missing_data]
            colors = ['#2ECC71', '#E74C3C']
            explode = (0.05, 0.05)
            
            ax1.pie(sizes, explode=explode, labels=['Complete', 'Incomplete'],
                   colors=colors, autopct='%1.1f%%', startangle=90)
            ax1.set_title(f'Data Completeness\n({completeness:.1f}% Complete)')
        
        # 2. ë°ì´í„° ìœ í˜•ë³„ ì»¤ë²„ë¦¬ì§€
        if verification.get('normalized_counts'):
            data_types = ['Schedules', 'Performances', 'Budgets', 'Overviews']
            coverage = []
            
            for table_key, display_name in zip(
                ['normalized_schedules', 'normalized_performances', 
                 'normalized_budgets', 'normalized_overviews'],
                data_types
            ):
                count = verification['normalized_counts'].get(table_key, 0)
                if total_projects > 0:
                    avg_per_project = count / total_projects
                    coverage.append(avg_per_project)
                else:
                    coverage.append(0)
            
            # ìˆ˜í‰ ë°” ì°¨íŠ¸
            y_pos = range(len(data_types))
            bars = ax2.barh(y_pos, coverage, color=['#3498DB', '#9B59B6', '#F39C12', '#1ABC9C'])
            
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(data_types)
            ax2.set_xlabel('Average Records per Project')
            ax2.set_title('Data Coverage by Type')
            ax2.grid(True, alpha=0.3, axis='x')
            
            # ê°’ í‘œì‹œ
            for i, bar in enumerate(bars):
                width = bar.get_width()
                ax2.text(width, bar.get_y() + bar.get_height()/2.,
                        f'{width:.1f}',
                        ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig(self.visualization_dir / 'data_completeness.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_pipeline_summary(self):
        """íŒŒì´í”„ë¼ì¸ ìš”ì•½ ì‹œê°í™”"""
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í†µê³„
        stages = ['JSON Files', 'Normalized\nRecords', 'DB Records\nLoaded']
        values = [
            self.pipeline_stats['json_files_processed'],
            self.pipeline_stats['normalized_records'],
            self.pipeline_stats['db_records_loaded']
        ]
        
        # ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜
        colors = ['#3498DB', '#2ECC71', '#F39C12']
        
        # ë°” ì°¨íŠ¸
        bars = ax.bar(stages, values, color=colors, alpha=0.8, edgecolor='black', linewidth=2)
        
        # ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height):,}',
                   ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        # ì œëª© ë° ë ˆì´ë¸”
        ax.set_title('Government Standard Data Pipeline Summary', 
                    fontsize=16, fontweight='bold', pad=20)
        ax.set_ylabel('Count', fontsize=12)
        ax.set_xlabel('Pipeline Stage', fontsize=12)
        
        # ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
        execution_time = (datetime.now() - self.pipeline_stats['start_time']).total_seconds()
        ax.text(0.02, 0.98, f'Execution Time: {execution_time:.1f} seconds',
               transform=ax.transAxes, fontsize=10,
               verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # ì˜¤ë¥˜ í‘œì‹œ
        if self.pipeline_stats['errors']:
            error_text = f"Errors: {len(self.pipeline_stats['errors'])}"
            ax.text(0.98, 0.98, error_text,
                   transform=ax.transAxes, fontsize=10,
                   verticalalignment='top', horizontalalignment='right',
                   color='red', fontweight='bold',
                   bbox=dict(boxstyle='round', facecolor='#FFE5E5', alpha=0.5))
        
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim(0, max(values) * 1.15)
        
        plt.tight_layout()
        plt.savefig(self.visualization_dir / 'pipeline_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_report(self, normalized_stats: Dict, db_verification: Dict):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š ì •ë¶€/ê³µê³µê¸°ê´€ í‘œì¤€ ë°ì´í„° ì²˜ë¦¬ ë³´ê³ ì„œ")
        report.append("=" * 80)
        report.append(f"ì‹¤í–‰ ì‹œê°„: {self.pipeline_stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ì†Œìš” ì‹œê°„: {(datetime.now() - self.pipeline_stats['start_time']).total_seconds():.1f}ì´ˆ")
        report.append("")
        
        report.append("ğŸ“ ì •ê·œí™” ê²°ê³¼:")
        report.append(f"  â€¢ ì²˜ë¦¬ëœ JSON íŒŒì¼: {normalized_stats.get('files_processed', 0)}ê°œ")
        report.append(f"  â€¢ ì´ ì •ê·œí™” ë ˆì½”ë“œ: {normalized_stats.get('total_records', 0):,}ê±´")
        
        if normalized_stats.get('records_by_type'):
            report.append("  â€¢ í…Œì´ë¸”ë³„ ë ˆì½”ë“œ:")
            for table, count in normalized_stats['records_by_type'].items():
                report.append(f"    - {table}: {count:,}ê±´")
        report.append("")
        
        report.append("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì ì¬:")
        report.append(f"  â€¢ ì´ ë‚´ì—­ì‚¬ì—…: {db_verification.get('total_sub_projects', 0)}ê°œ")
        report.append(f"  â€¢ ì›ë³¸ ë°ì´í„°: {db_verification.get('raw_data_count', 0)}ê±´")
        
        if db_verification.get('normalized_counts'):
            report.append("  â€¢ ì •ê·œí™” í…Œì´ë¸”:")
            for table, count in db_verification['normalized_counts'].items():
                report.append(f"    - {table}: {count:,}ê±´")
        report.append("")
        
        report.append("âœ… ë°ì´í„° ê²€ì¦:")
        missing_count = len(db_verification.get('missing_data', []))
        orphan_count = len(db_verification.get('orphan_records', {}))
        
        if missing_count == 0 and orphan_count == 0:
            report.append("  â€¢ ëª¨ë“  ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼ âœ“")
            self.pipeline_stats['verification_passed'] = True
        else:
            if missing_count > 0:
                report.append(f"  âš ï¸ ë°ì´í„° ëˆ„ë½ ì‚¬ì—…: {missing_count}ê°œ")
            if orphan_count > 0:
                report.append(f"  âš ï¸ ê³ ì•„ ë ˆì½”ë“œ í…Œì´ë¸”: {orphan_count}ê°œ")
        
        if self.pipeline_stats['errors']:
            report.append("")
            report.append("âŒ ì˜¤ë¥˜ ë°œìƒ:")
            for error in self.pipeline_stats['errors']:
                report.append(f"  - {error}")
        
        report.append("")
        report.append("ğŸ“Š ì‹œê°í™” íŒŒì¼:")
        report.append(f"  â€¢ ì €ì¥ ìœ„ì¹˜: {self.visualization_dir}")
        report.append("  â€¢ ìƒì„±ëœ ì°¨íŠ¸:")
        report.append("    - normalization_stats.png")
        report.append("    - db_load_stats.png")
        report.append("    - data_completeness.png")
        report.append("    - pipeline_summary.png")
        
        report.append("")
        report.append("=" * 80)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_text = "\n".join(report)
        report_file = self.visualization_dir / "processing_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # ì½˜ì†” ì¶œë ¥
        print(report_text)
        
        return report_text
    
    def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ì •ë¶€ í‘œì¤€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        try:
            # 1. ì •ê·œí™”
            normalized_stats = self.run_normalization()
            
            if normalized_stats['files_processed'] == 0:
                logger.error("âŒ ì •ê·œí™”í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ì ì¬
            db_verification = self.load_to_database()
            
            # 3. ì‹œê°í™”
            self.visualize_data(normalized_stats, db_verification)
            
            # 4. ë³´ê³ ì„œ ìƒì„±
            self.generate_report(normalized_stats, db_verification)
            
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            
            return self.pipeline_stats['verification_passed']
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            self.pipeline_stats['errors'].append(str(e))
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    pipeline = GovernmentDataPipeline()
    success = pipeline.run()
    
    if success:
        print("\nâœ… ì •ë¶€/ê³µê³µê¸°ê´€ í‘œì¤€ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  ë°ì´í„° ê²€ì¦ í†µê³¼!")
    else:
        print("\nâš ï¸ ì²˜ë¦¬ëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("visualization_government/processing_report.txt íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    return success


if __name__ == "__main__":
    success = main()